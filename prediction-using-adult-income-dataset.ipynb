{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10903076,"sourceType":"datasetVersion","datasetId":6776423}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prediction task is to determine whether a person's income is over $50,000 a year.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom statistics import mean\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom imblearn.over_sampling import SMOTE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T04:53:19.168196Z","iopub.execute_input":"2025-03-03T04:53:19.168543Z","iopub.status.idle":"2025-03-03T04:53:23.032144Z","shell.execute_reply.started":"2025-03-03T04:53:19.168499Z","shell.execute_reply":"2025-03-03T04:53:23.031083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/adult-income/adult_income.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T04:55:21.092527Z","iopub.execute_input":"2025-03-03T04:55:21.092944Z","iopub.status.idle":"2025-03-03T04:55:21.264824Z","shell.execute_reply.started":"2025-03-03T04:55:21.092914Z","shell.execute_reply":"2025-03-03T04:55:21.263842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"data.head(20)","metadata":{}},{"cell_type":"code","source":"data['income'].unique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['income'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Target Feature is **income column** and independent variables are the others.<br>\nThis is **binary classification. Label has only two kinds.**","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### we have 48842 rows and 15 columns. - before cleaning","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Find missing values\ndata.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we have missing values of \"workclass\", \"occupation\" and \"native_country\". ","metadata":{}},{"cell_type":"code","source":"data.duplicated().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.drop_duplicates(inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.duplicated().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['workclass'].unique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['occupation'].unique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['workclass'] = data.groupby(['education','age'])['workclass'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[data['workclass'].isna()]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I can't find any special things so I decided to fill new category which is 'unknown'.","metadata":{}},{"cell_type":"code","source":"data['workclass'].fillna('Unknown', inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['occupation'] = data.groupby(['workclass','education'])['occupation'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[data['occupation'].isna()]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['occupation'] = data.groupby('education')['occupation'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['native_country'] = data.groupby('race')['native_country'].transform(lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"edu_check = data.groupby(\"education\")[\"education_num\"].unique()\nedu_check.sort_values(ascending=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"edu_check = data.groupby(\"education\")[\"education_num\"].nunique()\nprint(edu_check[edu_check > 1])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"education_num is good matching with education column.<br>\nIt is continuous and the values are consistent.","metadata":{}},{"cell_type":"markdown","source":"### Finding Outliers","metadata":{}},{"cell_type":"code","source":"data['income'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After checking modeling performance, I will decide whether use SMOTE or not.","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[data['capital_gain']== 99999]['age'].sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In capital_gain column, the mean(1080) is much smaller than the standard deviation(7455), and all quartiles (25, 50, 75%) are 0.<br> The maximum value is likely to be an outlier.","metadata":{}},{"cell_type":"code","source":"captial_median = data['capital_gain'].median()\ndata.loc[data['capital_gain']== 99999, 'capital_gain'] = captial_median","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['capital_gain'].unique()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(data=data)","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[(data['hours_per_week']>70) & (data['age'] > 60)]['age'].sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"that doesn't make sense so I replaced median instead of that range.","metadata":{}},{"cell_type":"code","source":"hours_median = data['hours_per_week'].median()\ndata.loc[(data['age'] > 60) & (data['hours_per_week'] > 70), 'hours_per_week'] = hours_median","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# only numerical data type\ncolumns = data.select_dtypes(include=['number']).columns\n\nfor value in columns:\n    Q1 = data[value].quantile(0.25)\n    Q3 = data[value].quantile(0.75)\n    IQR = Q3 - Q1 \n    \n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # check number of outliers\n    outliers = data[(data[value] < lower_bound) | (data[value] > upper_bound)]\n    num_outliers = outliers.shape[0]\n    outlier_ratio = (num_outliers / len(data[value])) * 100\n    \n    print(f\"{value}: {outlier_ratio: .2f}%\")","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fig : total, axes : each graph, 1 means one row\nfig, axes = plt.subplots(1, len(columns), figsize=(15,5))\n\nfig.suptitle('Plot the histograms for numerical data')\n\n# create each histogram \nfor i, col in enumerate(columns):\n    sns.histplot(data[col], bins=10, kde=True, ax=axes[i])\n    axes[i].set_title(f'Histogram of {col}') #subtitle\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dealing with outliers of 'hours per week' column\nQ1 = data['hours_per_week'].quantile(0.25)\nQ3 = data['hours_per_week'].quantile(0.75)\nIQR = Q3 - Q1 \n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# set median instead of outlier\nmedian_value = data['hours_per_week'].median()\ndata.loc[(data['hours_per_week'] < lower_bound) | (data['hours_per_week'] > upper_bound), 'hours_per_week'] = median_value","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The distribution of capital_gain is too skewed, so replacing outliers with the median might reduce data reliability.<br>\nSo I found another way to deal with outliers. <br>","metadata":{}},{"cell_type":"code","source":"# add new category (binary) for Trees model\ndata['has_capital_gain'] = (data['capital_gain'] > 0).astype(int)\n# change log for reducing scale for linear model\ndata['capital_gain_log'] = np.log1p(data['capital_gain'])  # log(x+1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = data.select_dtypes(include=['number']).columns\n\n# fig : total, axes : each graph, 1 means one row\nfig, axes = plt.subplots(1, len(columns), figsize=(15,5))\n\nfig.suptitle('Plot the histograms for numerical data')\n\n# create each histogram \nfor i, col in enumerate(columns):\n    sns.histplot(data[col], bins=10, kde=True, ax=axes[i])\n    axes[i].set_title(f'Histogram of {col}') #subtitle\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for value in columns:\n    Q1 = data[value].quantile(0.25)\n    Q3 = data[value].quantile(0.75)\n    IQR = Q3 - Q1 \n    \n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # check number of outliers\n    outliers = data[(data[value] < lower_bound) | (data[value] > upper_bound)]\n    num_outliers = outliers.shape[0]\n    outlier_ratio = (num_outliers / len(data[value])) * 100\n    \n    print(f\"{value}: {outlier_ratio: .2f}%\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I reduced around **2%** outliers of 'hours_per_week' coulmn. the left outliers will be scaled using feature scaling.","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.duplicated().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.drop_duplicates(inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.duplicated().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### after cleaning, we have 48735 rows and 17 columns","metadata":{}},{"cell_type":"markdown","source":"### Category Data type to Numerical Data type","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for education, it already has numerical column which is education_num.\ncat_col = ['workclass','marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in cat_col:\n    print(f\"{col} : {data[col].nunique()}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"la_encoder = LabelEncoder()\ndata['workclass_en'] = la_encoder.fit_transform(data['workclass'])\ndata['marital_status_en'] = la_encoder.fit_transform(data['marital_status'])\ndata['occupation_en'] = la_encoder.fit_transform(data['occupation'])\ndata['relationship_en'] = la_encoder.fit_transform(data['relationship'])\ndata['native_country_en'] = la_encoder.fit_transform(data['native_country'])\ndata['race_trees'] = la_encoder.fit_transform(data['race'])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# one-hot encoding\ndata = pd.get_dummies(data, columns=['race'], drop_first=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['sex'].value_counts(dropna=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"for \"sex\" and \"income\" column, I will recreate new binaries columns.","metadata":{}},{"cell_type":"code","source":"data['sex'] = data['sex'].map({'Male': 0, 'Female': 1})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['income'].value_counts(dropna=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['income'] = data['income'].map({'<=50K': 0, '>50K': 1})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['income'].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## visualize the correlation matrix.","metadata":{}},{"cell_type":"markdown","source":"when I perform correlation, we need to pick the column which is encoded.","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"feature scaling is not neseccery for **trees model**.","metadata":{}},{"cell_type":"code","source":"tree_data = data[['age', 'fnlwgt', 'education_num', 'sex',\n                        'has_capital_gain', 'capital_loss', 'hours_per_week',\n                        'workclass_en', 'marital_status_en', 'occupation_en',\n                        'relationship_en', 'native_country_en', 'race_trees', 'income' ]]\ntree_data.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_data = data[['age', 'fnlwgt', 'education_num', 'sex',\n                        'capital_gain_log', 'capital_loss', 'hours_per_week',\n                        'workclass_en', 'marital_status_en', 'occupation_en',\n                        'relationship_en', 'native_country_en', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'income' ]]\ncorr = numeric_data.corr()\ncorr","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Correlation matrix\nplt.figure(figsize = (16,16))\nsns.heatmap(corr, annot=True, cmap='Reds', fmt='.2f')","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### the top 3 features that are highly correlated with the target feature","metadata":{}},{"cell_type":"code","source":"# correlation with target feature('income')\n# top3\ntop_features = corr['income'].abs().sort_values(ascending=False).iloc[1:4] \ntop_features","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For linear model data set, **\"education_num\", \"capital_gain_log\", \"age\"** columns are highly correlated with target features.","metadata":{}},{"cell_type":"code","source":"corr_tree = tree_data.corr()\ncorr_tree","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Correlation matrix\nplt.figure(figsize = (16,16))\nsns.heatmap(corr_tree, annot=True, cmap='Reds', fmt='.2f')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# correlation with target feature('income')\n# top3\ntop_features = corr_tree['income'].abs().sort_values(ascending=False).iloc[1:4] \ntop_features","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For Trees model data set, **\"education_num\", \"relationshop_en\", \"capital_gain\"** columns are highly correlated with target features.","metadata":{}},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{}},{"cell_type":"code","source":"X = numeric_data.drop(['income'], axis=1)\nY = numeric_data['income']","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_co = X\nY_co = Y","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I have a lot of outliers so, I decided to use robust scaler.","metadata":{}},{"cell_type":"code","source":"ro_scaler = RobustScaler()\nX_train_scaled = ro_scaler.fit_transform(X_train)\nX_test_scaled = ro_scaler.transform(X_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = pd.DataFrame(X_train_scaled, columns=X.columns)\nX_test = pd.DataFrame(X_test_scaled, columns=X.columns)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ML modeling","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression(random_state=16, max_iter=1000)\nlogreg.fit(X_train, Y_train)\n\n# probablity for auc-roc cure\nlg_probs = logreg.predict_proba(X_test)\nlg_probs","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_train = logreg.predict(X_train)\ny_pred = logreg.predict(X_test)\n\nconf_matrix = confusion_matrix(Y_test, y_pred)\nconf_matrix","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- True Negatives (TN) = 6952<br>\nThe model correctly predicted 6952 people as earning ≤50K.\n\n- False Positives (FP) = 461<br>\nThe model incorrectly predicted 461 people as earning >50K when they actually earn ≤50K (Type I error).\n\n- False Negatives (FN) = 1295<br>\nThe model incorrectly predicted 1295 people as earning ≤50K when they actually earn >50K (Type II error).\n\n- True Positives (TP) = 1039<br>\nThe model correctly predicted 1039 people as earning >50K.","metadata":{}},{"cell_type":"code","source":"# probablity\n\nprint(f\"Train Accuracy score for Logistic Regression: {accuracy_score(Y_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy score for Logistic Regression: {accuracy_score(Y_test, y_pred):.2f}\")\n\n\ntarget_names = ['<=50K', '>50K']\nprint(classification_report(Y_test, y_pred, target_names = target_names))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Key Observation\n    1. Major issue: The model struggles to identify >50K earners, missing 55% of them due to the low recall of 0.45.\n- Final Verdict\n    1. Good accuracy (82%) with no overfitting.\n    2. Severe class imbalance issue: The model is biased toward ≤50K and struggles with >50K earners (low recall of 0.45).\n    3. High False Negatives for >50K → Many high-income individuals are misclassified as low-income.","metadata":{}},{"cell_type":"markdown","source":"### KNN","metadata":{}},{"cell_type":"code","source":"error_rates = []\nk_values = range(1, 31)\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, Y_train)\n    Y_knn_pred = knn.predict(X_test)\n    error = 1 - accuracy_score(Y_test, Y_knn_pred)\n    error_rates.append(error)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plot the elbow curve to find the optimal value of k\nplt.figure(figsize=(20, 5))\nplt.plot(k_values, error_rates, marker='o', linestyle='dashed', color='b', markersize=8)\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Error Rate')\nplt.title('Elbow Method to Find Optimal k')\nplt.xticks(np.arange(1, 31))\nplt.grid(True)\nplt.show()","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The error rate is the least for k = 26.**","metadata":{}},{"cell_type":"code","source":"#Performing KNN with optimal value of k\nknn = KNeighborsClassifier(n_neighbors=26)\nknn.fit(X_train, Y_train)\n\nY_knn_probs = knn.predict_proba(X_test)\nY_knn_probs\n\ny_pred_train = knn.predict(X_train)\nY_knn_pred = knn.predict(X_test)\n\nconf_matrix = confusion_matrix(Y_test, Y_knn_pred)\nconf_matrix","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-\tTrue Negatives (TN) = 6922<br>\nThe model correctly predicted 6922 people as earning ≤50K.\n-\tFalse Positives (FP) = 491<br>\nThe model incorrectly predicted 491 people as earning >50K, when they actually earn ≤50K (Type I error).\n-\tFalse Negatives (FN) = 1020<br>\nThe model incorrectly predicted 1020 people as earning ≤50K, when they actually earn >50K (Type II error).\n-\tTrue Positives (TP) = 1314<br>\nThe model correctly predicted 1314 people as earning >50K.\n","metadata":{}},{"cell_type":"code","source":"print(f\"Train Accuracy score for KNN: {accuracy_score(Y_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy score for KNN: {accuracy_score(Y_test, Y_knn_pred):.2f}\")\n\ntarget_names = ['<=50K', '>50K']\nprint(classification_report(Y_test, Y_knn_pred, target_names = target_names))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Overall Observation\n    1. The model struggles with >50K earners, misclassifying 44% of them as ≤50K.\n- Final Verdict\n    1. Good overall accuracy (84%) and strong recall for ≤50K (93%).\n    2. Class imbalance issue: The model struggles with the >50K class, identifying only 56% of them correctly.\n    3. High False Negatives for >50K: Many high earners are misclassified as ≤50K.","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"# split data for trees model\nX_t = tree_data.drop(['income'], axis=1)\nY_t = tree_data['income']\n\nX_t_train, X_t_test, Y_t_train, Y_t_test = train_test_split(X_t, Y_t, test_size=0.2, random_state=42)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Find the best value for max_depth in decision tree\nscores = []\ndepths = range(1, 21)\nfor d in depths:\n    score = cross_val_score(DecisionTreeClassifier(criterion= 'entropy', max_depth=d), X_t_train, Y_t_train, cv=5)\n    avg_score = mean(score)\n    scores.append(avg_score)\n\nbest_depth = depths[np.argmax(scores)]\nprint(f\"Best max_depth: {best_depth}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dt_clr_opt = DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=7)\ndt_clr_opt.fit(X_t_train, Y_t_train)\n\n#predict\ny_pred_train = dt_clr_opt.predict(X_t_train)\ny_pred_test = dt_clr_opt.predict(X_t_test)\n\nconf_matrix = confusion_matrix(Y_t_test, y_pred_test)\nconf_matrix","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- True Negatives (TN) = 6914<br>\nThe model correctly predicted 6914 people as earning ≤50K.\n- False Positives (FP) = 499<br>\nThe model incorrectly predicted 499 people as earning >50K, when they actually earn ≤50K (Type I error).\n- False Negatives (FN) = 1096<br>\nThe model incorrectly predicted 1096 people as earning ≤50K, when  they actually earn >50K (Type II error).\n- True Positives (TP) = 1238<br>\nThe model correctly predicted 1238 people as earning >50K.","metadata":{}},{"cell_type":"code","source":"print(f\"Train Accuracy:, {accuracy_score(Y_t_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy:, {accuracy_score(Y_t_test, y_pred_test):.2f}\")\ntarget_names = ['<=50K', '>50K']\nprint(\"\\nClassification Report:\\n\", classification_report(Y_t_test, y_pred_test, target_names=target_names))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Overall Observation\n    1. The model struggles to correctly identify all >50K earners, misclassifying 47% of them as ≤50K.\n- Final Verdict\n    1. Good overall accuracy (84%) and strong recall for ≤50K (93%).\n    2. Imbalance problem: The model struggles with the >50K class, only identifying 53% of them correctly.\n    3. False Negatives for >50K: Many high earners are misclassified as ≤50K.","metadata":{}},{"cell_type":"markdown","source":"### Plot the decision tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\nimport os\nos.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin/\"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export the decision tree to DOT format\ndot_data = StringIO()\n\nexport_graphviz(\n    dt_clr_opt,\n    out_file=dot_data, \n    filled=True,  \n    rounded=True,  \n    special_characters=True,  \n    feature_names=X_t_train.columns, \n    class_names=['Low income','High income']\n)\n\n# Generate graph\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(\"decision_tree.png\")  # Save as PNG\n\n# Display the tree\nImage(filename=\"decision_tree.png\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SVM","metadata":{}},{"cell_type":"code","source":"# to check what kernel should I use\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_train)  # dimensionality reduction\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=Y_train, palette='coolwarm', alpha=0.7)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"PCA Projection of Data\")\nplt.legend(title=\"Class\")\nplt.show()","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I couldn't find linear separation so I would use rbf kernel.","metadata":{}},{"cell_type":"code","source":"svm = SVC() #Model building\n\nparam_grid = {\"C\": [1, 10, 100, 200, 300]}\n\n# GridSearchCV\ngrid_search = GridSearchCV(svm, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\ngrid_search.fit(X_train, Y_train)\n\nprint(f\"Best C: {grid_search.best_params_['C']}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"svm_300 = SVC(random_state=42, C=300, probability=True)\nsvm_300.fit(X_train, Y_train)\n\nY_svm_probs = svm_300.predict_proba(X_test)\n\ny_pred_train = svm_300.predict(X_train)\ny_pred_test = svm_300.predict(X_test)\n\nconf_matrix = confusion_matrix(Y_test, y_pred_test)\nconf_matrix","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-\tTrue Negatives (TN) = 7072 <br>\nThe model correctly predicted 7072 people as earning ≤50K.\n-\tFalse Positives (FP) = 341<br>\nThe model incorrectly predicted 341 people as earning >50K, when they actually earn ≤50K (Type I error).\n-\tFalse Negatives (FN) = 1699<br>\nThe model incorrectly predicted 1699 people as earning ≤50K, when they actually earn >50K (Type II error).\n-\tTrue Positives (TP) = 635<br>\nThe model correctly predicted 635 people as earning >50K.","metadata":{}},{"cell_type":"code","source":"# train and test accuracy\nprint(f\"Train Accuracy:, {accuracy_score(Y_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy:, {accuracy_score(Y_test, y_pred_test):.2f}\")\n\ntarget_names = ['<=50K', '>50K']\nprint(classification_report(Y_test, y_pred_test, target_names = target_names))\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Overall Observation\n    1. The model fails to identify 73% of actual >50K earners, which is a serious class imbalance problem.\n- Final Verdict\n    1. Moderate overall accuracy (79%), but extremely low recall for >50K (27%), meaning most high earners are not correctly identified.\n    2. Severe class imbalance issue: The model performs well for ≤50K (95% recall) but fails to generalize for >50K.\n    3. Extremely high False Negatives for >50K: A large proportion of high-income individuals are misclassified as ≤50K, making this model unreliable for detecting high earners.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n\nparam_grid = {\n    \"n_estimators\": [50, 100, 200],  # trees\n    \"max_depth\": [10, 20, None],  \n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4] \n}\n# for better one\ngrid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=2)\ngrid_search.fit(X_t_train, Y_t_train)\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Accuracy:\", grid_search.best_score_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the best model\nbest_rf = grid_search.best_estimator_\n\ny_pred_train = best_rf.predict(X_t_train)\ny_pred_test = best_rf.predict(X_t_test)\n\nprint(f\"Train Accuracy: {accuracy_score(Y_t_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy: {accuracy_score(Y_t_test, y_pred_test):.2f}\")\nprint()\n# Confusion Matrix\nconf_matrix = confusion_matrix(Y_t_test, y_pred_test)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint()\n# Classification Report\nprint(\"Classification Report:\\n\", classification_report(Y_t_test, y_pred_test, target_names=[\"≤50K\", \">50K\"]))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- True Negatives (TN) = 6926<br>\nThe model correctly predicted 6926 people as earning ≤50K.\n- False Positives (FP) = 487<br>\nThe model incorrectly predicted 487 people as earning >50K, when they actually earn ≤50K (Type I error).\n- False Negatives (FN) = 956<br>\nThe model incorrectly predicted 956 people as earning ≤50K, when they actually earn >50K (Type II error).\n- True Positives (TP) = 1378<br>\nThe model correctly predicted 1378 people as earning >50K.","metadata":{}},{"cell_type":"markdown","source":"- Overall Observations\n    1. The model fails to correctly identify 41% of actual >50K earners, which can be problematic in applications where high-income classification is crucial.\n    2. High FN count for >50K: Many high earners are misclassified, suggesting class imbalance issues.\n- Final Verdict\n    1. Decent accuracy (85%) and high recall for ≤50K earners (93%).\n    2.  Slight overfitting (train accuracy 91% vs test accuracy 85%).\n    3.  Moderate class imbalance issue: The model still underperforms for the >50K class (low recall of 59%).\n    4.  Many False Negatives for >50K: The model fails to identify 41% of actual high earners, which reduces its reliability for classifying >50K incomes.","metadata":{}},{"cell_type":"markdown","source":"## AUC-ROC curve","metadata":{}},{"cell_type":"code","source":"# Graph for only positive\nY_lr_probs = lg_probs[:, 1]\nY_knn_probs = Y_knn_probs[:, 1]\nY_dt_probs = dt_clr_opt.predict_proba(X_t_test)[:, 1]\nY_svm_probs = Y_svm_probs[:, 1]\nY_rf_probs = best_rf.predict_proba(X_t_test)[:, 1]\n\n# Compute ROC curve and AUC for all the models\n# Logistic Regression\nfpr_lr, tpr_lr, _ = roc_curve(Y_test, Y_lr_probs)\nauc_lr = auc(fpr_lr, tpr_lr)\n# KNN\nfpr_knn, tpr_knn, _ = roc_curve(Y_test, Y_knn_probs)\nauc_knn = auc(fpr_knn, tpr_knn)\n# Decision Trees\nfpr_dt, tpr_dt, _ = roc_curve(Y_t_test, Y_dt_probs)\nauc_dt = auc(fpr_dt, tpr_dt)\n# SVM\nfpr_svm, tpr_svm, _ = roc_curve(Y_test, Y_svm_probs)\nauc_svm = auc(fpr_svm, tpr_svm)\n# Random Forest\nfpr_rf, tpr_rf, _ = roc_curve(Y_t_test, Y_rf_probs)\nauc_rf = auc(fpr_rf, tpr_rf)\n\n# Plot both ROC curves\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_lr, tpr_lr, color='blue', label=f'Logistic Regression (AUC = {auc_lr:.2f})')\nplt.plot(fpr_knn, tpr_knn, color='red', linestyle='dashed', label=f'KNN (k=12) (AUC = {auc_knn:.2f})')\nplt.plot(fpr_dt, tpr_dt, color='green', linestyle='dashdot', label=f'Decision Tree (AUC = {auc_dt:.2f})')\nplt.plot(fpr_svm, tpr_svm, color='orange', linestyle='dashed', label=f'SVM (AUC = {auc_svm:.2f})')\nplt.plot(fpr_rf, tpr_rf, color='pink', linestyle='dashdot', label=f'Random Forest (AUC = {auc_rf:.2f})')\n\n# Random guess line\nplt.plot([0, 1], [0, 1], color='black', linestyle='dotted', label='Random Guess')\n\n# Labels and Title\nplt.xlabel(\"False Positive Rate (FPR)\")\nplt.ylabel(\"True Positive Rate (TPR)\")\nplt.title(\"ROC Curve Comparison: All Classification Models\")\nplt.grid(True)\nplt.legend(loc='best') \nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The best model is the Random Forest.","metadata":{}},{"cell_type":"markdown","source":"## AFTER SMOTE","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"Y.value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Apply SMOTE\nsmote = SMOTE(sampling_strategy='minority')\nX, Y = smote.fit_resample(X, Y)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y.value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**After SMOTE, I have 74114 rows and 16 columns**","metadata":{}},{"cell_type":"code","source":"#Apply SMOTE for Trees data set\nsmote = SMOTE(sampling_strategy='minority')\nX_t, Y_t = smote.fit_resample(X_t, Y_t)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Y_t.value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_t.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**After SMOTE, I have 74114 rows and 13 columns for Trees Data set**","metadata":{}},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# split data for Linear model\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nro_scaler = RobustScaler()\nX_train_scaled = ro_scaler.fit_transform(X_train)\nX_test_scaled = ro_scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train_scaled, columns=X.columns)\nX_test = pd.DataFrame(X_test_scaled, columns=X.columns)\n\nX_train.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split data for trees model\n\nX_t_train, X_t_test, Y_t_train, Y_t_test = train_test_split(X_t, Y_t, test_size=0.2, random_state=42)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ML modeling","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression(random_state=16, max_iter=2000)\nlogreg.fit(X_train, Y_train)\n\n# probablity for auc-roc cure\nlg_probs = logreg.predict_proba(X_test)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_train = logreg.predict(X_train)\ny_pred = logreg.predict(X_test)\n\nconf_matrix = confusion_matrix(Y_test, y_pred)\nconf_matrix","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- True Negatives (TN) = 5825 <br>\nThe model correctly predicted 5825 people as earning ≤50K.\n- False Positives (FP) = 1640<br>\nThe model incorrectly predicted 1640 people as earning >50K, when they actually earn ≤50K (Type I error).\n- False Negatives (FN) = 1384<br>\nThe model incorrectly predicted 1384 people as earning ≤50K, when they actually earn >50K (Type II error).\n- True Positives (TP) = 5974<br>\nThe model correctly predicted 5974 people as earning >50K.","metadata":{}},{"cell_type":"code","source":"# probablity\n\nprint(f\"Train Accuracy score for Logistic Regression: {accuracy_score(Y_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy score for Logistic Regression: {accuracy_score(Y_test, y_pred):.2f}\")\n\n\ntarget_names = ['<=50K', '>50K']\nprint(classification_report(Y_test, y_pred, target_names = target_names))","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Observations\n\t1.\tBalanced Performance → The model performs similarly across both classes, meaning it does not strongly favor one over the other.\n\t2.\tRecall for >50K is slightly higher than ≤50K (0.82 vs. 0.78) → The model is slightly better at capturing high-income individuals.\n\t3.\tPrecision and Recall are close, leading to a well-balanced F1-score → No extreme trade-off between false positives and false negatives.\n\n- Final Verdict\n    1. The model performs well with an overall F1-score of 0.80, indicating a reliable classification performance.\n    2. Some improvement can be made in recall for ≤50K (0.78), which means the model might be missing some low-income individuals.\n\t","metadata":{}},{"cell_type":"markdown","source":"### KNN","metadata":{}},{"cell_type":"code","source":"error_rates = []\nk_values = range(1, 101)\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, Y_train)\n    Y_knn_pred = knn.predict(X_test)\n    error = 1 - accuracy_score(Y_test, Y_knn_pred)\n    error_rates.append(error)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plot the elbow curve to find the optimal value of k\nplt.figure(figsize=(20, 5))\nplt.plot(k_values, error_rates, marker='o', linestyle='dashed', color='b', markersize=8)\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Error Rate')\nplt.title('Elbow Method to Find Optimal k')\nplt.xticks(np.arange(1, 101))\nplt.grid(True)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The error rate is the least for k = 3.**","metadata":{}},{"cell_type":"code","source":"#Performing KNN with optimal value of k\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, Y_train)\n\nY_knn_probs = knn.predict_proba(X_test)\nY_knn_probs\n\ny_pred_train = knn.predict(X_train)\nY_knn_pred = knn.predict(X_test)\n\nconf_matrix = confusion_matrix(Y_test, Y_knn_pred)\nconf_matrix","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- True Negatives (TN) = 6300 <br>\nThe model correctly predicted 6300 people as earning ≤50K.\n- False Positives (FP) = 1165<br>\nThe model incorrectly predicted 1165 people as earning >50K, when they actually earn ≤50K (Type I error).\n- False Negatives (FN) = 1076<br>\nThe model incorrectly predicted 1076 people as earning ≤50K, when they actually earn >50K (Type II error).\n- True Positives (TP) = 6282<br>\nThe model correctly predicted 6282 people as earning >50K.","metadata":{}},{"cell_type":"code","source":"print(f\"Train Accuracy score for KNN: {accuracy_score(Y_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy score for KNN: {accuracy_score(Y_test, Y_knn_pred):.2f}\")\n\ntarget_names = ['≤50K', '>50K']\nprint(classification_report(Y_test, Y_knn_pred, target_names = target_names))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Observations\n\t1.\tBalanced Classification → The model performs equally well on both classes, with no significant bias toward one.\n\t2.\tPotential Overfitting → The 7% accuracy gap suggests that the model may be slightly overfitting. Reducing the number of neighbors (k) or applying regularization techniques might improve generalization.\n\t3.\tStrong Overall Performance → With an F1-score of 0.85, the model is reliable and effective for classification.\n\n- Final Verdict\n\t1. The KNN model performs well, with balanced precision, recall, and F1-scores across both classes.\n    2. There is some overfitting, but it is not severe. Fine-tuning hyperparameters (e.g., increasing k) could help improve test performance.\n\t","metadata":{}},{"cell_type":"code","source":"cv_scores = cross_val_score(knn, X_train, Y_train, cv=5)\nprint(f\"Cross-validation mean accuracy: {cv_scores.mean():.2f}\" )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**I initially thought this model was overfitting, but after comparing the test accuracy with the cross-validation results, I found them to be same, so I concluded that there is no issue.**","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"#Find the best value for max_depth in decision tree\nscores = []\ndepths = range(1, 21)\nfor d in depths:\n    score = cross_val_score(DecisionTreeClassifier(criterion= 'entropy', max_depth=d), X_t_train, Y_t_train, cv=5)\n    avg_score = mean(score)\n    scores.append(avg_score)\n\nbest_depth = depths[np.argmax(scores)]\nprint(f\"Best max_depth: {best_depth}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dt_clr_opt = DecisionTreeClassifier(random_state=42, criterion='entropy', max_depth=12)\ndt_clr_opt.fit(X_t_train, Y_t_train)\n\n#predict\ny_pred_train = dt_clr_opt.predict(X_t_train)\ny_pred_test = dt_clr_opt.predict(X_t_test)\n\nconf_matrix = confusion_matrix(Y_t_test, y_pred_test)\nconf_matrix","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- True Negatives (TN) = 6136<br>\nThe model correctly predicted 6136 people as earning ≤50K.\n- False Positives (FP) = 1329<br>\nThe model incorrectly predicted 1329 people as earning >50K when they actually earn ≤50K (Type I error).\n- False Negatives (FN) = 995<br>\nThe model incorrectly predicted 995 people as earning ≤50K when they actually earn >50K (Type II error).\n- True Positives (TP) = 6363<br>\nThe model correctly predicted 6363 people as earning >50K.","metadata":{}},{"cell_type":"code","source":"print(f\"Train Accuracy:, {accuracy_score(Y_t_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy:, {accuracy_score(Y_t_test, y_pred_test):.2f}\")\ntarget_names = ['≤50K', '>50K']\nprint(\"\\nClassification Report:\\n\", classification_report(Y_t_test, y_pred_test, target_names=target_names))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Observations\n\t1.\tBalanced Performance → The model performs similarly across both classes, meaning it does not strongly favor one over the other.\n\t2.\tRecall for >50K is slightly higher than ≤50K (0.86 vs. 0.83) → The model is slightly better at capturing high-income individuals.\n\t3.\tPrecision and Recall are close, leading to a well-balanced F1-score → No extreme trade-off between false positives and false negatives.\n\n- Final Verdict\n    1. The model performs well with an overall F1-score of 0.84, indicating a reliable classification performance.\n    2. Some improvement can be made in recall for ≤50K (0.83), which means the model might be missing some low-income individuals.\n\t","metadata":{}},{"cell_type":"code","source":"cv_scores = cross_val_score(dt_clr_opt, X_t_train, Y_t_train, cv=5)\nprint(f\"Cross-validation mean accuracy: {cv_scores.mean():.2f}\" )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**I initially thought this model was overfitting, but after comparing the test accuracy with the cross-validation results, I found them to be same, so I concluded that there is no issue.**","metadata":{}},{"cell_type":"markdown","source":"### Plot the decision tree","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# Export the decision tree to DOT format\ndot_data = StringIO()\n\nexport_graphviz(\n    dt_clr_opt,\n    out_file=dot_data, \n    filled=True,  \n    rounded=True,  \n    special_characters=True,  \n    feature_names=X_t_train.columns, \n    class_names=['Low income','High income']\n)\n\n# Generate graph\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(\"decision_tree_SMOTE.png\")  # Save as PNG\n\n# Display the tree\nImage(filename=\"decision_tree_SMOTE.png\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import resample\n\nX_train_sample, Y_train_sample = resample(X_train, Y_train, n_samples=5000, random_state=42)  # 5000개만 사용\n\ngrid_search.fit(X_train_sample, Y_train_sample)\n\nsvm = SVC() #Model building\n\nparam_grid = {\"C\": [10, 100, 200, 300]}\n\n# GridSearchCV\ngrid_search = GridSearchCV(svm, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\ngrid_search.fit(X_train_sample, Y_train_sample)\n\nprint(f\"Best C: {grid_search.best_params_['C']}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"svm_300 = SVC(random_state=42, C=300, probability=True)\nsvm_300.fit(X_train, Y_train)\n\nY_svm_probs = svm_300.predict_proba(X_test)\n\ny_pred_train = svm_300.predict(X_train)\ny_pred_test = svm_300.predict(X_test)\n\nconf_matrix = confusion_matrix(Y_test, y_pred_test)\nconf_matrix","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- True Negatives (TN) = 5521\nThe model correctly predicted 5521 people as earning ≤50K.\n- False Positives (FP) = 1944\nThe model incorrectly predicted 1944 people as earning >50K, when they actually earn ≤50K (Type I error).\n- False Negatives (FN) = 1094\nThe model incorrectly predicted 1094 people as earning ≤50K, when they actually earn >50K (Type II error).\n- True Positives (TP) = 6264\nThe model correctly predicted 6264 people as earning >50K.","metadata":{}},{"cell_type":"code","source":"# train and test accuracy\nprint(f\"Train Accuracy:, {accuracy_score(Y_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy:, {accuracy_score(Y_test, y_pred_test):.2f}\")\n\ntarget_names = ['≤50K', '>50K']\nprint(classification_report(Y_test, y_pred_test, target_names = target_names))\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Key Observations\n\t1.\tNo Overfitting → Since train and test accuracy are the same (80%), the model generalizes well.\n\t2.\tClass Imbalance Impact\n    \t-\tThe model is slightly biased toward the >50K class since it has a higher recall (0.85 vs. 0.74).\n    \t-\tThis means the model misses more actual ≤50K cases (higher False Negatives) than it does for >50K cases.\n\t3.\tBetter Recall for >50K Class → The model is more effective at capturing >50K earners (85%) than ≤50K earners (74%).\n\t4.\tTrade-off Between Precision & Recall\n    \t-\tThe ≤50K class has higher precision (0.83) but lower recall (0.74), meaning it is good at confirming ≤50K cases but tends to miss some.\n    \t-\tThe >50K class has lower precision (0.76) but higher recall (0.85), meaning it captures most >50K cases but with slightly more False Positives.\n- Final Verdict\n    1. Balanced model but slightly favors the >50K class\n    2. Good generalization (no overfitting)\n    3. Might need adjustment to improve recall for ≤50K class ","metadata":{}},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n\nparam_grid = {\n    \"n_estimators\": [50, 100, 200],  # trees\n    \"max_depth\": [10, 20, None],  \n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4] \n}\n# for better one\ngrid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=2)\ngrid_search.fit(X_t_train, Y_t_train)\n\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Accuracy:\", grid_search.best_score_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the best model\nbest_rf = grid_search.best_estimator_\n\ny_pred_train = best_rf.predict(X_t_train)\ny_pred_test = best_rf.predict(X_t_test)\n\nprint(f\"Train Accuracy: {accuracy_score(Y_t_train, y_pred_train):.2f}\")\nprint(f\"Test Accuracy: {accuracy_score(Y_t_test, y_pred_test):.2f}\")\nprint()\n# Confusion Matrix\nconf_matrix = confusion_matrix(Y_t_test, y_pred_test)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint()\n# Classification Report\nprint(\"Classification Report:\\n\", classification_report(Y_t_test, y_pred_test, target_names=[\"≤50K\", \">50K\"]))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- True Negatives (TN) = 6254\nThe model correctly predicted 6254 people as earning ≤50K.\n\n- False Positives (FP) = 1211 (Type I error)\nThe model incorrectly predicted 1211 people as earning >50K when they actually earn ≤50K.\n\n- False Negatives (FN) = 737 (Type II error)\nThe model incorrectly predicted 737 people as earning ≤50K when they actually earn >50K.\n\n- True Positives (TP) = 6621\nThe model correctly predicted 6621 people as earning >50K.","metadata":{}},{"cell_type":"markdown","source":"- Key Observations\n    1. Potential Overfitting -> the 7% accuarcy gap suggests that the model may be slightly overfitting.\n    2. Strong Overall Performance -> with an F1-score of 0.87, the model is reliable and effective for classification.\n    3. More False Positives (1211) than False Negatives (737) → The model overestimates high-income individuals more often than it misses them.","metadata":{}},{"cell_type":"code","source":"cv_scores = cross_val_score(best_rf, X_t_train, Y_t_train, cv=5)\nprint(\"Cross-validation mean accuracy:\", cv_scores.mean())","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**I initially thought this model was overfitting, but after comparing the test accuracy with the cross-validation results, I found them to be similar, so I concluded that there is no issue.**","metadata":{}},{"cell_type":"markdown","source":"## AUC-ROC curve","metadata":{}},{"cell_type":"code","source":"# Graph for only positive\nY_lr_probs = lg_probs[:, 1]\nY_knn_probs = Y_knn_probs[:, 1]\nY_dt_probs = dt_clr_opt.predict_proba(X_t_test)[:, 1]\nY_svm_probs = Y_svm_probs[:, 1]\nY_rf_probs = best_rf.predict_proba(X_t_test)[:, 1]\n\n# Compute ROC curve and AUC for all the models\n# Logistic Regression\nfpr_lr, tpr_lr, _ = roc_curve(Y_test, Y_lr_probs)\nauc_lr = auc(fpr_lr, tpr_lr)\n# KNN\nfpr_knn, tpr_knn, _ = roc_curve(Y_test, Y_knn_probs)\nauc_knn = auc(fpr_knn, tpr_knn)\n# Decision Trees\nfpr_dt, tpr_dt, _ = roc_curve(Y_t_test, Y_dt_probs)\nauc_dt = auc(fpr_dt, tpr_dt)\n# SVM\nfpr_svm, tpr_svm, _ = roc_curve(Y_test, Y_svm_probs)\nauc_svm = auc(fpr_svm, tpr_svm)\n# Random Forest\nfpr_rf, tpr_rf, _ = roc_curve(Y_t_test, Y_rf_probs)\nauc_rf = auc(fpr_rf, tpr_rf)\n\n# Plot both ROC curves\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_lr, tpr_lr, color='blue', label=f'Logistic Regression (AUC = {auc_lr:.2f})')\nplt.plot(fpr_knn, tpr_knn, color='red', linestyle='dashed', label=f'KNN (k=12) (AUC = {auc_knn:.2f})')\nplt.plot(fpr_dt, tpr_dt, color='green', linestyle='dashdot', label=f'Decision Tree (AUC = {auc_dt:.2f})')\nplt.plot(fpr_svm, tpr_svm, color='orange', linestyle='dashed', label=f'SVM (AUC = {auc_svm:.2f})')\nplt.plot(fpr_rf, tpr_rf, color='pink', linestyle='dashdot', label=f'Random Forest (AUC = {auc_rf:.2f})')\n\n# Random guess line\nplt.plot([0, 1], [0, 1], color='black', linestyle='dotted', label='Random Guess')\n\n# Labels and Title\nplt.xlabel(\"False Positive Rate (FPR)\")\nplt.ylabel(\"True Positive Rate (TPR)\")\nplt.title(\"ROC Curve Comparison: All Classification Models\")\nplt.grid(True)\nplt.legend(loc='best') \nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Conclusion: Model Performance Before and After Addressing Class Imbalance\n\n**Before Handling Class Imbalance**\n- Random Forest (AUC = 0.90) performed the best, followed closely by KNN (AUC = 0.89) and Decision Tree (AUC = 0.88).\n- SVM (AUC = 0.83) had the lowest AUC, suggesting it struggled more compared to the other models.\n\n**After Applying SMOTE (Handling Class Imbalance)**\n- Random Forest showed the highest improvement, increasing from AUC 0.90 to 0.95, indicating it benefited significantly from SMOTE.\n- KNN and Decision Tree also improved to AUC 0.91, making them strong contenders.\n- Logistic Regression and SVM improved from 0.84 → 0.88 and 0.83 → 0.88, respectively, showing that balancing the dataset positively impacted their performance.\n\n**Key Observations & Interpretation**<br>\n1. All models improved after handling class imbalance, suggesting that SMOTE helped address the bias toward the majority class.<br>\n2. Random Forest consistently performed the best both before and after SMOTE, making it the most robust model in this scenario.<br>\n3. Decision Tree and KNN saw notable improvements, indicating they were sensitive to class imbalance and benefited from synthetic data augmentation.<br>\n4. SVM and Logistic Regression had the lowest AUC initially but showed considerable improvement after SMOTE, meaning they required balanced data to perform well.\n\n**Final Verdict**\n- Best Model Before Class Imbalance → Random Forest (AUC = 0.90)\n- Best Model After SMOTE → Random Forest (AUC = 0.95)\n- Most Improved Models → Decision Tree & KNN (AUC increased to 0.91)\n- Overall Recommendation → Random Forest remains the best choice, but Decision Tree and KNN are also strong contenders after handling class imbalance.","metadata":{}}]}